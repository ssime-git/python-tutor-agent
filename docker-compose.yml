services:
  app:
    build: .
    ports:
      - "8000:8000"
    volumes:
      - ./app:/app
      - ./data:/data
    env_file:
      - .env
    environment:
      - PYTHONUNBUFFERED=1
      - LANGCHAIN_TRACING_V2=true
      - LANGCHAIN_PROJECT=python-tutor-agent
      - LANGCHAIN_ENDPOINT=https://api.smith.langchain.com
      - LANGCHAIN_API_KEY=${LANGCHAIN_API_KEY}
    depends_on:
      - chroma
      - code-executor
      - litellm

  chroma:
    image: chromadb/chroma:latest
    ports:
      - "8001:8000"
    volumes:
      - ./data/chroma:/chroma/chroma
      
  code-executor:
    build: ./code-executor
    ports:
      - "8002:8080"
    volumes:
      - ./code-executor/tmp:/tmp
    environment:
      - EXECUTION_TIMEOUT=5
    entrypoint: >
      sh -c "mkdir -p /tmp/executions &&
             chmod 777 /tmp/executions &&
             uvicorn app:app --host 0.0.0.0 --port 8080"

  # LiteLLM server
  litellm:
    image: ghcr.io/berriai/litellm:main-latest
    ports:
      - "8008:4000"
    environment:
      - PORT=4000
      - GOOGLE_API_KEY=${GOOGLE_API_KEY}
      - DEFAULT_MODEL=gemini-2.0-flash
      # Uncomment if you want to use a master key for authentication
      # - LITELLM_MASTER_KEY=${LITELLM_MASTER_KEY}
      # Add verbose logging to debug issues
      - LITELLM_LOG_LEVEL=debug
    volumes:
      - ./litellm-config:/app/config
    command: ["--config", "/app/config/config.yaml", "--port", "4000"]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:4000/health"]
      interval: 10s
      timeout: 5s
      retries: 5